import os
from fastapi import FastAPI, HTTPException
from fastapi.responses import FileResponse
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_community.embeddings import HuggingFaceEmbeddings  # ë¬´ë£Œ ë¡œì»¬ ì„ë² ë”©!
from typing import List, Optional

app = FastAPI()

# 1. CORS ì„¤ì • (ë¦¬ì•¡íŠ¸ì™€ í†µì‹  í—ˆìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# â˜…â˜…â˜… ì—¬ê¸°ì— êµ¬ê¸€ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš” â˜…â˜…â˜…
os.environ["GOOGLE_API_KEY"] = "AIzaSyCCNnKlAaCCMRdh_OETXbdEMLmCrBLR52g"

# ì „ì—­ ë³€ìˆ˜
vectorstore = None
loaded_documents_info = []  # PDF ë©”íƒ€ë°ì´í„° ì €ì¥

# PDF íŒŒì¼ëª…ì—ì„œ êµ­ê°€ì™€ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ í•¨ìˆ˜
def extract_metadata(filename):
    """íŒŒì¼ëª…ì—ì„œ êµ­ê°€ì™€ ì¹´í…Œê³ ë¦¬ë¥¼ ìŠ¤ë§ˆíŠ¸í•˜ê²Œ ì¶”ì¶œ"""
    filename_lower = filename.lower()
    
    # êµ­ê°€ ê°ì§€
    if "canada" in filename_lower:
        country = "Canada"
    elif "france" in filename_lower or "franÃ§aise" in filename_lower or "snbc" in filename_lower or "carbone" in filename_lower:
        country = "France"
    elif "germany" in filename_lower or "klima" in filename_lower or "german" in filename_lower:
        country = "Germany"
    elif "african" in filename_lower or "afdb" in filename_lower:
        country = "Africa"
    elif "un " in filename_lower or "global" in filename_lower or "ndc" in filename_lower:
        country = "Global"
    else:
        country = "Global"
    
    # ì¹´í…Œê³ ë¦¬ ê°ì§€
    if "agri" in filename_lower or "food" in filename_lower:
        category = "Agriculture"
    elif "energy" in filename_lower or "grid" in filename_lower or "renewable" in filename_lower:
        category = "Energy"
    elif "emission" in filename_lower or "carbon" in filename_lower or "climate" in filename_lower or "klima" in filename_lower:
        category = "Climate"
    elif "health" in filename_lower:
        category = "Health"
    elif "supply" in filename_lower or "logistics" in filename_lower:
        category = "Supply Chain"
    else:
        category = "Policy"
    
    # ë¬¸ì„œ íƒ€ì… ê°ì§€
    if "report" in filename_lower:
        doc_type = "Report"
    elif "plan" in filename_lower or "strategy" in filename_lower or "strategie" in filename_lower:
        doc_type = "Strategy"
    elif "brief" in filename_lower:
        doc_type = "Brief"
    elif "act" in filename_lower or "law" in filename_lower:
        doc_type = "Legislation"
    else:
        doc_type = "Document"
    
    # ë…„ë„ ì¶”ì¶œ ì‹œë„
    import re
    year_match = re.search(r'20\d{2}', filename)
    year = year_match.group() if year_match else "2023"
    
    return country, category, doc_type, year

# 2. PDF ë¡œë“œ ë° ë©”íƒ€ë°ì´í„° íƒœê¹… í•¨ìˆ˜
def load_and_tag_documents():
    global vectorstore, loaded_documents_info
    documents = []
    loaded_documents_info = []  # ì´ˆê¸°í™”
    
    # ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©
    data_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)), "data")
    
    # í´ë” ì—†ìœ¼ë©´ ìƒì„±
    if not os.path.exists(data_folder):
        os.makedirs(data_folder)
        print("ğŸ“ 'data' í´ë”ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. PDF íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.")
        return

    print("ğŸ”„ PDF ë¡œë”© ë° íƒœê·¸ ë¶„ì„ ì‹œì‘...")
    
    files = [f for f in os.listdir(data_folder) if f.endswith(".pdf")]
    
    if not files:
        print("âš ï¸ data í´ë”ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ëª¨ë“  íŒŒì¼ ë¡œë“œ (ë¡œì»¬ ì„ë² ë”© ì‚¬ìš©í•˜ë¯€ë¡œ í• ë‹¹ëŸ‰ ê±±ì • ì—†ìŒ)
    print(f"ğŸ“„ ì´ {len(files)}ê°œ PDF íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.")

    for idx, filename in enumerate(files):
        # ìŠ¤ë§ˆíŠ¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        country_tag, category_tag, doc_type, year = extract_metadata(filename)
        
        try:
            loader = PyPDFLoader(os.path.join(data_folder, filename))
            docs = loader.load()
            
            # ì²« í˜ì´ì§€ì—ì„œ excerpt ì¶”ì¶œ
            excerpt = docs[0].page_content[:200] + "..." if docs else "No content available."
            
            # ì½ì€ ë¬¸ì„œì— íƒœê·¸(ê¼¬ë¦¬í‘œ) ë‹¬ê¸°
            for doc in docs:
                doc.metadata["country"] = country_tag
                doc.metadata["category"] = category_tag
                doc.metadata["source"] = filename
            
            documents.extend(docs)
            
            # ë¬¸ì„œ ì •ë³´ ì €ì¥ (í”„ë¡ íŠ¸ì—”ë“œìš©)
            loaded_documents_info.append({
                "id": f"DOC-{idx+1:03d}",
                "filename": filename,
                "title": filename.replace(".pdf", "").replace("-", " ").replace("_", " "),
                "country": country_tag,
                "category": category_tag,
                "type": doc_type,
                "year": year,
                "pages": len(docs),
                "excerpt": excerpt
            })
            
            print(f"   ğŸ‘‰ ë¡œë“œ ì„±ê³µ: {filename} -> [êµ­ê°€:{country_tag}, ì¹´í…Œê³ ë¦¬:{category_tag}]")
        except Exception as e:
            print(f"   âŒ ë¡œë“œ ì‹¤íŒ¨ ({filename}): {e}")

    # ë¬¸ì„œê°€ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ DB êµ¬ì¶•
    if documents:
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(documents)
        
        # ë¬´ë£Œ ë¡œì»¬ ì„ë² ë”© (HuggingFace) - API í• ë‹¹ëŸ‰ ê±±ì • ì—†ìŒ!
        print("ğŸ”„ ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘ (ì²˜ìŒì—” ë‹¤ìš´ë¡œë“œë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŒ)...")
        embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
        # ChromaDBì— ì €ì¥
        vectorstore = Chroma.from_documents(splits, embeddings)
        print(f"âœ… ì´ {len(documents)} í˜ì´ì§€, {len(loaded_documents_info)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ. RAG ì¤€ë¹„ ë!")
    else:
        print("âš ï¸ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

# 3. ìš”ì²­ ë°ì´í„° ëª¨ë¸
class ChatRequest(BaseModel):
    query: str
    country: str = "Global"
    scenario: str = "General"
    language: str = "en"      # en, ko, fr, de

class InsightRequest(BaseModel):
    """AI Strategic Briefingìš© ì¸ì‚¬ì´íŠ¸ ìš”ì²­"""
    country: str = "Canada"
    scenario: str = "Health"  # Agri, Energy, Supply, Health
    weather_data: dict = {}   # í˜„ì¬ ê¸°ìƒ ë°ì´í„°
    language: str = "en"      # en, ko, fr, de

# [ìƒˆë¡œìš´ í•µì‹¬ API] AI-Driven Insight ìƒì„±
@app.post("/insight")
def generate_insight(req: InsightRequest):
    """
    í˜„ì¬ ê¸°ìƒ ë°ì´í„° + RAG ë¬¸ì„œ ê²€ìƒ‰ â†’ LLMì´ ì •ì±… ì¸ì‚¬ì´íŠ¸ ìƒì„±
    """
    global vectorstore
    
    # ì–¸ì–´ë³„ ì„¤ì •
    LANG_CONFIG = {
        "en": {"name": "English", "respond_in": "Respond entirely in English."},
        "ko": {"name": "í•œêµ­ì–´", "respond_in": "ëª¨ë“  ì‘ë‹µì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”."},
        "fr": {"name": "FranÃ§ais", "respond_in": "RÃ©pondez entiÃ¨rement en franÃ§ais."},
        "de": {"name": "Deutsch", "respond_in": "Antworten Sie vollstÃ¤ndig auf Deutsch."}
    }
    lang_cfg = LANG_CONFIG.get(req.language, LANG_CONFIG["en"])
    
    if not vectorstore:
        return {
            "insight": "ì •ì±… ë¬¸ì„œê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
            "signal": "N/A",
            "status": "ERROR",
            "action": "ì„œë²„ë¥¼ ì¬ì‹œì‘í•˜ì„¸ìš”",
            "sources": [],
            "key_points": []
        }
    
    # ì‹œë‚˜ë¦¬ì˜¤ë³„ ê²€ìƒ‰ ì¿¼ë¦¬ ë° ë¶„ì„ í¬ì¸íŠ¸ ì„¤ì •
    scenario_config = {
        "Agri": {
            "search_query": f"agriculture climate policy soil moisture drought food security {req.country}",
            "signal_key": "soilMoisture",
            "signal_format": lambda d: f"Soil Moisture: {(d.get('soilMoisture', 0.3) * 100):.1f}%",
            "focus": "ë†ì—… ì •ì±…, ê°€ë­„ ëŒ€ì‘, ì‹ëŸ‰ ì•ˆë³´"
        },
        "Energy": {
            "search_query": f"energy policy renewable solar wind grid climate {req.country}",
            "signal_key": "solarRad",
            "signal_format": lambda d: f"Solar Radiation: {d.get('solarRad', 0):.1f} MJ/mÂ²",
            "focus": "ì—ë„ˆì§€ ì „í™˜, ì¬ìƒì—ë„ˆì§€, íƒ„ì†Œ ë°°ì¶œ"
        },
        "Supply": {
            "search_query": f"supply chain logistics climate risk port infrastructure {req.country}",
            "signal_key": "gust",
            "signal_format": lambda d: f"Wind Gusts: {d.get('gust', 0):.1f} km/h",
            "focus": "ë¬¼ë¥˜ ë¦¬ìŠ¤í¬, ê³µê¸‰ë§ ë³µì›ë ¥, ì¸í”„ë¼"
        },
        "Health": {
            "search_query": f"public health climate heat wave cold weather policy {req.country}",
            "signal_key": "feelTemp",
            "signal_format": lambda d: f"Apparent Temp: {d.get('feelTemp', 0):.1f}Â°C",
            "focus": "ê³µì¤‘ ë³´ê±´, ê¸°ì˜¨ ê·¹í•œ ëŒ€ì‘, ì·¨ì•½ê³„ì¸µ ë³´í˜¸"
        }
    }
    
    config = scenario_config.get(req.scenario, scenario_config["Health"])
    weather = req.weather_data or {}
    
    # RAG ê²€ìƒ‰ ì‹¤í–‰ - ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰
    search_kwargs = {
        "k": 8,  # ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰
        "filter": {
            "$or": [
                {"country": req.country},
                {"country": "Global"}
            ]
        }
    }
    
    try:
        # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)
        relevant_docs = retriever.get_relevant_documents(config["search_query"])
        
        # ğŸ”¥ Deep RAG: í˜ì´ì§€, ì„¹ì…˜ ì •ë³´ë¥¼ í¬í•¨í•œ ì •ë°€ ì»¨í…ìŠ¤íŠ¸ êµ¬ì¶•
        detailed_citations = []
        context_text = ""
        
        for idx, doc in enumerate(relevant_docs[:6]):
            source = doc.metadata.get('source', 'Unknown')
            page = doc.metadata.get('page', 'N/A')
            content = doc.page_content[:1000]
            
            # ì„¹ì…˜/í—¤ë”© ì¶”ì¶œ ì‹œë„ (ë¬¸ì„œ ë‚´ êµ¬ì¡° íŒŒì•…)
            lines = content.split('\n')
            section_hint = ""
            for line in lines[:5]:
                if any(kw in line.lower() for kw in ['chapter', 'section', 'article', 'ì¡°', 'ì¥', 'í•­', 'teil', 'chapitre']):
                    section_hint = line.strip()[:80]
                    break
            
            citation = {
                "doc_id": idx + 1,
                "source": source,
                "page": page,
                "section": section_hint or "Main Content",
                "excerpt": content[:300] + "..." if len(content) > 300 else content
            }
            detailed_citations.append(citation)
            
            context_text += f"""
[Document {idx+1}]
- Source: {source}
- Page: {page}
- Section: {section_hint or 'N/A'}
- Content:
{content}

---
"""
        
        sources = list(set([doc.metadata.get('source', 'Unknown') for doc in relevant_docs]))
        
        # LLMìœ¼ë¡œ ì¸ì‚¬ì´íŠ¸ ìƒì„± (gemini-2.0-flash ì‚¬ìš©)
        llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash")
        
        # ğŸ”¥ Chain of Thought í”„ë¡¬í”„íŠ¸ - ì¶”ë¡  ê³¼ì •ì„ ëª…ì‹œì ìœ¼ë¡œ ìš”ì²­
        prompt = f"""You are a G7 climate policy analysis AI with deep expertise.
{lang_cfg['respond_in']}

## Current Situation
- Country: {req.country}
- Analysis Scenario: {req.scenario} ({config['focus']})
- Current Weather Data: 
  * Apparent Temp: {weather.get('feelTemp', 'N/A')}Â°C
  * Actual Temp: {weather.get('realTemp', 'N/A')}Â°C
  * Soil Moisture: {(weather.get('soilMoisture', 0.3) * 100):.1f}%
  * Precipitation: {weather.get('rain', 0)}mm
  * Solar Radiation: {weather.get('solarRad', 0)} MJ/mÂ²
  * Wind Gusts: {weather.get('gust', 0)} km/h
  * Snowfall: {weather.get('snow', 0)}cm

## Reference Policy Documents (with page numbers)
{context_text}

## CRITICAL INSTRUCTIONS
1. **Micro-Citation Required**: When referencing a policy, you MUST cite the exact document SOURCE FILENAME, page, and section. Example: "According to 'Canada Emission Reduction Plan.pdf', Page 15, Section 3.2..."
2. **Chain of Thought Required**: Show your reasoning step by step. Explain WHY you reached your conclusion.
3. **IMPORTANT**: Always include the FULL source filename in citations, not just document numbers.
4. {lang_cfg['respond_in']}

## Response Format (JSON only):
{{
  "status": "ALERT or CAUTION or STABLE or OPTIMAL",
  "headline": "One-line key insight in {lang_cfg['name']}",
  
  "reasoning_chain": [
    {{
      "step": 1,
      "type": "DATA_OBSERVATION",
      "content": "Observation about current weather data (e.g., 'Soil moisture is at 20.6%, which is below the typical threshold of 30%')"
    }},
    {{
      "step": 2,
      "type": "POLICY_LOOKUP",
      "content": "What the policy document says - MUST mention the actual document filename",
      "citation": {{"doc_id": 3, "page": "42", "section": "4.1", "source": "Canada Emission Reduction Plan.pdf"}}
    }},
    {{
      "step": 3,
      "type": "INFERENCE",
      "content": "Logical inference connecting data and policy (e.g., 'Since current moisture (20.6%) < threshold (25%), drought protocol should be activated')"
    }},
    {{
      "step": 4,
      "type": "CONCLUSION",
      "content": "Final conclusion with recommended action"
    }}
  ],
  
  "micro_citations": [
    {{
      "doc_id": 1,
      "source": "filename.pdf",
      "page": "15",
      "section": "Section 3.2",
      "quote": "Exact quote or paraphrase from the document (max 100 chars)",
      "relevance": "Why this citation matters for current situation"
    }}
  ],
  
  "analysis": "Detailed analysis connecting weather data and policy documents (3-4 sentences, in {lang_cfg['name']})",
  "action": "Specific recommended action in {lang_cfg['name']}",
  "key_points": ["Key point 1", "Key point 2", "Key point 3"],
  "confidence_score": 0.85,
  "policy_relevance": "Which policy document is most relevant and why (in {lang_cfg['name']})"
}}

Output ONLY valid JSON. No markdown, no explanations, just the JSON object.
"""
        
        response = llm.invoke(prompt)
        response_text = response.content.strip()
        
        # JSON íŒŒì‹± ì‹œë„
        import json
        import re
        
        # JSON ë¸”ë¡ ì¶”ì¶œ
        json_match = re.search(r'\{[\s\S]*\}', response_text)
        if json_match:
            insight_data = json.loads(json_match.group())
        else:
            insight_data = {
                "status": "STABLE",
                "headline": "ë¶„ì„ ì¤‘...",
                "analysis": response_text[:300],
                "action": "ëª¨ë‹ˆí„°ë§ ì§€ì†",
                "key_points": [],
                "policy_relevance": "",
                "reasoning_chain": [],
                "micro_citations": []
            }
        
        # ğŸ”¥ Deep RAG ì‘ë‹µ - Chain of Thought + Micro-Citation í¬í•¨
        return {
            "signal": config["signal_format"](weather),
            "status": insight_data.get("status", "STABLE"),
            "headline": insight_data.get("headline", ""),
            "analysis": insight_data.get("analysis", ""),
            "action": insight_data.get("action", ""),
            "key_points": insight_data.get("key_points", []),
            "policy_relevance": insight_data.get("policy_relevance", ""),
            
            # ğŸ†• Chain of Thought - AI ì¶”ë¡  ê³¼ì •
            "reasoning_chain": insight_data.get("reasoning_chain", []),
            
            # ğŸ†• Micro-Citations - ì •ë°€ ì¸ìš©
            "micro_citations": insight_data.get("micro_citations", []),
            
            # ğŸ†• ì‹ ë¢°ë„ ì ìˆ˜
            "confidence_score": insight_data.get("confidence_score", 0.7),
            
            # ê¸°ì¡´ í•„ë“œ
            "sources": sources,
            "detailed_citations": detailed_citations,  # ì›ë³¸ ë¬¸ì„œ ì •ë³´
            "scenario": req.scenario,
            "country": req.country
        }
        
    except Exception as e:
        print(f"Insight generation error: {e}")
        import traceback
        traceback.print_exc()
        return {
            "signal": config["signal_format"](weather),
            "status": "ERROR",
            "headline": "Analysis error occurred",
            "analysis": str(e),
            "action": "System check required",
            "key_points": [],
            "reasoning_chain": [],
            "micro_citations": [],
            "confidence_score": 0,
            "sources": [],
            "scenario": req.scenario,
            "country": req.country
        }

# 4. ì±„íŒ… API (ë©”íƒ€ë°ì´í„° í•„í„°ë§ ì ìš©)
@app.post("/chat")
def chat(req: ChatRequest):
    global vectorstore
    
    # DBê°€ ë¹„ì–´ìˆìœ¼ë©´ ì•ˆë‚´ ë©”ì‹œì§€ ë°˜í™˜
    if not vectorstore:
        return {
            "answer": "ì„œë²„ì— ë¡œë“œëœ ì •ì±… ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. backend/data í´ë”ì— PDFë¥¼ ë„£ê³  ì„œë²„ë¥¼ ì¬ì‹œì‘í•´ì£¼ì„¸ìš”.",
            "sources": []
        }
    
    # [í•µì‹¬] ê²€ìƒ‰ í•„í„°: ë‚´ êµ­ê°€ ë¬¸ì„œ OR ê¸€ë¡œë²Œ ë¬¸ì„œë§Œ ì°¾ê¸°
    search_kwargs = {
        "k": 4, # ìƒìœ„ 4ê°œ ë¬¸ì„œ ì°¸ì¡°
        "filter": {
            "$or": [
                {"country": req.country}, # ì„ íƒí•œ êµ­ê°€ (ì˜ˆ: Canada)
                {"country": "Global"}     # ë˜ëŠ” ê³µí†µ ë¬¸ì„œ
            ]
        }
    }
    
    # ì–¸ì–´ë³„ ì„¤ì •
    LANG_CONFIG = {
        "en": {"name": "English", "respond_in": "Respond entirely in English."},
        "ko": {"name": "í•œêµ­ì–´", "respond_in": "ëª¨ë“  ì‘ë‹µì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”."},
        "fr": {"name": "FranÃ§ais", "respond_in": "RÃ©pondez entiÃ¨rement en franÃ§ais."},
        "de": {"name": "Deutsch", "respond_in": "Antworten Sie vollstÃ¤ndig auf Deutsch."}
    }
    lang_cfg = LANG_CONFIG.get(req.language, LANG_CONFIG["en"])
    
    llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash")
    
    # í”„ë¡¬í”„íŠ¸: ì„ íƒëœ ì–¸ì–´ë¡œ ì „ë¬¸ì ì¸ ë‹µë³€ ìœ ë„
    template = f"""
    You are a G7 policy advisor.
    Currently analyzing the '{req.scenario}' scenario for '{req.country}'.
    {lang_cfg['respond_in']}
    
    Based on the provided Context, answer the question clearly in {lang_cfg['name']}.
    If the information is not in the context, say you don't know. Don't make things up.
    
    Context: {{context}}
    
    Question: {{question}}
    
    Answer (in {lang_cfg['name']}):
    """
    PROMPT = PromptTemplate.from_template(template)

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vectorstore.as_retriever(search_kwargs=search_kwargs),
        return_source_documents=True,
        chain_type_kwargs={"prompt": PROMPT}
    )
    
    try:
        # ì§ˆë¬¸ ë˜ì§€ê¸°
        result = qa_chain.invoke({"query": req.query})
        
        # ì°¸ê³ í•œ ë¬¸ì„œ ì¶œì²˜ ëª©ë¡ ë§Œë“¤ê¸°
        sources = list(set([doc.metadata['source'] for doc in result['source_documents']]))
        
        return {
            "answer": result['result'],
            "sources": sources
        }
    except Exception as e:
        return {
            "answer": f"ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", 
            "sources": []
        }

# 5. ë¬¸ì„œ ëª©ë¡ API - Policy Libraryìš©
@app.get("/documents")
def get_documents():
    """Policy Libraryì—ì„œ ì‚¬ìš©í•  ë¬¸ì„œ ëª©ë¡ ë°˜í™˜"""
    return {"documents": loaded_documents_info}

# 6. PDF íŒŒì¼ ì„œë¹™ - View Documentìš©
@app.get("/pdf/{filename:path}")
def get_pdf(filename: str):
    """PDF íŒŒì¼ì„ ë¸Œë¼ìš°ì €ì—ì„œ ë³¼ ìˆ˜ ìˆë„ë¡ ì„œë¹™"""
    pdf_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)), "data")
    file_path = os.path.join(pdf_folder, filename)
    if os.path.exists(file_path):
        return FileResponse(file_path, media_type="application/pdf", filename=filename)
    return {"error": "File not found"}

# ì„œë²„ ì¼œì§ˆ ë•Œ ì‹¤í–‰
@app.on_event("startup")
def startup():
    load_and_tag_documents()

if __name__ == "__main__":
    import uvicorn
    # 0.0.0.0ìœ¼ë¡œ ì—´ì–´ì„œ ì™¸ë¶€ ì ‘ì† í—ˆìš©
    uvicorn.run(app, host="0.0.0.0", port=8000)
